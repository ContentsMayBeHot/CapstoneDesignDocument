\chapter{Methods}


\section{Design}
%-------------------------------------------%

The project consists of several core components: a SerpentAI game agent plugin, a dataset management program, a frame-action synchronizer, a replay parser, and a machine learning program.

The first stage of the project is data preprocessing. This consists of converting the replay files into a labeled dataset. Replays serve as the foundation for the project's data component. They are plain text files with a \textit{.roa} extension. They contain all of the information needed to reproduce a match in \textit{Rivals of Aether}. The first few lines contain meta data and information about the match rules, such as the time stamp, which level the match took place on, the game version, lives count, et cetera. Subsequent lines provide information about each player such as their name, character, and an enumerated list of every action they took during the entirety of the match. Actions are enumerated by their offset from the first frame. For example if a player presses the the attack button on frame 216 and releases it 15 frames later, then the actions will contain something that looks like the following: \textit{216A231a}. The capital \textit{A} indicates that the button was pressed and the lowercase \textit{a} shows the button being released.

The replay parser converts all of the information stored in a replay file into a convenient object-oriented wrapper. Rules and meta data are stored in attribute, enumerations provide human-readable names for action codes, and methods allow for the conversion of player action data into more usefully formatted matrices. The dataset management program uses the replay parser to process all of the replays. It moves them into folders corresponding with their game versions. This results in the following directory structure:

%% Source: https://tex.stackexchange.com/a/270761
\begin{forest}
    for tree={
        font=\ttfamily,
        grow'=0,
        child anchor=west,
        parent anchor=south,
        anchor=west,
        calign=first,
        edge path={
            \noexpand\path [draw, \forestoption{edge}]
            (!u.south west) +(7.5pt,0) |- node[fill,inner sep=1.25pt] {} (.child anchor)\forestoption{edge label};
        },
        before typesetting nodes={
            if n=1
            {insert before={[,phantom]}}
            {}
        },
        fit=band,
        before computing xy={l=15pt},
    }
    [replays
        [00\_15\_07]
        [00\_15\_08]
        [00\_15\_09]
        [00\_15\_10]
        [00\_15\_11]
        [01\_00\_02]
        [01\_00\_03]
        [01\_00\_05]
        [01\_01\_02]
        [01\_02\_01]
        [01\_02\_02]
    ]
\end{forest}

In the example above, \textit{00\_15\_07} refers to game version 0.15.7, \textit{01\_02\_01} to 1.2.1, and so on. The game version matters because replays are not compatible across different game versions. If a replay has a stage or character that does not exist in the running version of the game, then the replays menu will actually crash when loaded. Thus, with all of the replay files sorted in the manner shown above, frame collection is made possible.

Frame collection is one of the tasks fulfilled by the SerpentAI game agent. This consists of playing back each replay for the currently installed version of the game, one at a time, while dumping the frame buffer to the file system via serialization. The game agent reliably controls playback using simple timed input sequences. The duration of a replay is not explicitly given by a replay file, but can be estimated with sufficient accuracy by identifying the highest frame index and then converting that number to seconds. For example, the final action for player 1 may happen on frame 8,961, while the final action for player 2 is on 8,954. Given a frame rate of 60 frames per second, this means that player 1 stopped playing after 149.35 seconds and player 2 after 149.23. Thus, the match duration was almost certainly 150 seconds. Knowing this, the game agent can count to 150 while collecting frames. After the match has ended, the agent can stop collecting frames and then use timed inputs to initiate playback of the next replay. Over time, a file structure resembling this will develop:

%% Source: https://tex.stackexchange.com/a/270761
\begin{forest}
    for tree={
        font=\ttfamily,
        grow'=0,
        child anchor=west,
        parent anchor=south,
        anchor=west,
        calign=first,
        edge path={
            \noexpand\path [draw, \forestoption{edge}]
            (!u.south west) +(7.5pt,0) |- node[fill,inner sep=1.25pt] {} (.child anchor)\forestoption{edge label};
        },
        before typesetting nodes={
            if n=1
            {insert before={[,phantom]}}
            {}
        },
        fit=band,
        before computing xy={l=15pt},
    }
    [replays
        [00\_15\_07]
        [...]
        [01\_02\_02]
        [frames
            [2017-11-07-234316128959]
            [...]
            [2017-11-07-234241093349
                [0.npy]
                [...]
                [4475.npy]
            ]
        ]
        [labels
            [2017-11-07-234316128959]
            [...]
            [2017-11-07-234241093349
                [roa\_1.npy]
                [roa\_2.npy]
            ]
        ]
    ]
\end{forest}

\begin{figure}
    \caption{Game agent data flow}
    \centering
    \includegraphics[scale = 0.75]{dataflow.png} \\
\end{figure}

Alongside the version folders there are now two new folders called \textit{frames} and \textit{labels}. Each frame collected by the agent gets dumped into its own \textit{x.npy} file located in replays/frames/replay-title. The \textit{x} in the file name is replaced by a number representing its offset from the first frame. An offset of 1, for example, describes the second frame, while an offset of 99 describes the 100th frame. Meanwhile, each player's actions are converted to a matrix format and then dumped into \textit{replays/labels/replay-title} as \textit{roa\_x.npy}, where \textit{x} identifies the player (e.g. player 1 or player 2).

Actually tracking the replays and moving them into the game's replays folder is handled by the dataset manager. During collection, only 1 replay is allowed in the game's replays folder at a time. Thus, the dataset manager tracks which replays are visited and, when asked, deletes the contents of the replays folder before copying over the next replay. When the manager reports that there are no replays left for the selected game version, the collection agent knows to stop.

When collection is over, the dataset manager can be used to sort all of the data in the \textit{frames} and \textit{labels} folders across the training and testing sets. Invoking this task will result in the following changes to the file structure:

%% Source: https://tex.stackexchange.com/a/270761
\begin{forest}
    for tree={
        font=\ttfamily,
        grow'=0,
        child anchor=west,
        parent anchor=south,
        anchor=west,
        calign=first,
        edge path={
            \noexpand\path [draw, \forestoption{edge}]
            (!u.south west) +(7.5pt,0) |- node[fill,inner sep=1.25pt] {} (.child anchor)\forestoption{edge label};
        },
        before typesetting nodes={
            if n=1
            {insert before={[,phantom]}}
            {}
        },
        fit=band,
        before computing xy={l=15pt},
    }
    [replays
        [00\_15\_07]
        [...]
        [01\_02\_02]
        [frames]
        [labels]
        [sets
            [testing
                [frames]
                [labels]
            ]
            [training
                [frames]
                [labels]
            ]
        ]
    ]
\end{forest}

By default, 80 percent of the replays will go into the \textit{training} folder and the remaining 20 percent into \textit{testing}. The machine learning program includes a specialized loader designed to read datasets in the form shown above. In other words, it can load the training set from \textit{replays/sets/training} and the testing set from \textit{replays/sets/testing}. For either set, the loader gathers a list to every file path in the \textit{frames} and \textit{labels} folders. It then starts a worker thread which loads the frame and label data into a queue, which allows the neural network to run its training and testing loops without getting blocked.

There are several expensive operations that the worker thread handles. First, it loads all of the frames from the file system into memory. Then it explodes the label data. Exploding the labels is necessary because there is no guarantee that the frames that were collected are the frames on which actions took place. For example, consider a situation in which frames 6, 17, and 25 were collected by the game agent. According to the replay file, player 1 pressed the jump key in frame 3, released it on frame 15, and then pressed it again on frame 82. The replay file contains no references to the collected frames. One might assume that there is nothing to do; the collected frames are useless. However, it is entirely possible to extrapolate the button states for all of the frames that were collected, even if the replay file contains no references to them. The jump key was pressed on frame 3 and released on frame 15, which means that it was in a pressed state on frame 6. Given that the jump button is not pressed again until 82, it's safe to assume that it is in an unpressed state on frames 17 and 25. If this process is repeated for all 9 buttons across the entire replay, then all of the collected frames can be given accurate labels.

%-------------------------------------------%

\section{Frameworks}
%-------------------------------------------%

Our project uses a software stack comprised of the following modules and libraries: SerpentAI, Keras, TensorFlow, and the Anaconda Distribution for Python 3. We chose to use Python 3 as our primary programming language for its flexibility, intuitive syntax, and data processing capabilities. Python is also helpfully compatible with all of the other software listed above. Furthermore, the {\it Anaconda distribution} provides us with a large assortment of Python modules, some of which {\it SerpentAI} cites as dependencies \cite{SerpentAI}. This allows us to more easily work from within a Windows environment, which in turn gives us the most straightforward and stable access to GPU computation via NVIDIA proprietary drivers. TensorFlow serves as the project's backbone by actually creating the neural network in addition to performing the necessary computations \cite{TensorFlow}, with Keras allowing us to perform rapid prototyping in TensorFlow via its abstracted API \cite{Keras}.

All experiments will take place on a pair of 64-bit Windows 10 machines with Ubuntu subsystems. There are several reasons to justify this decision. Firstly, we wanted to ensure that we would have the best possible experience when dealing with GPU computation, and NVIDIA simply has a longer and better track record for supporting Windows than it does for Linux. Secondly, we wanted to avoid the potential pitfall of our platform restricting us to a smaller library of target games. Our two top candidates were Quake and Rivals of Aether because both of these games support replays, or demos; however, of those two games, only Quake natively supports Linux, and ultimately we selected Rivals of Aether. Lastly, and with regards to using our own computers for both development and testing: we made the decision to abstain from distributed or cloud computing chiefly to save costs, but also as an aesthetic choice based in our desire to push our own hardware to its absolute limits. Despite all of the above justifications for using Windows, it remains to be said that our project cannot live without Linux. SerpentAI requires a Redis database, so in order for that to work we use an Ubuntu subsystem; this is the official recommendation from the SerpentAI developers \cite{SerpentAI}.

%-------------------------------------------%



\section{Algorithms}
%-------------------------------------------%

The artificial neural network takes 2 inputs and produces 1 output. These are called \textit{x}, \textit{y\textsubscript{1}}, and \textit{y}, respectively.

The primary input, \textit{x}, is a video clip consisting of 60 gray-scale frames, where each frame is 80 by 45 pixels with 1 color channel. The auxiliary input, \textit{y\textsubscript{1}}, contains the most recent actions taken by the player; these are represented by an array containing 9 integers, where each index of the array is associated with a different action and contains either a 1 if the player took that action or a 0 if not.

The video clips are processed by a stack of four 2D convolutional LSTM layers that are buffered with 2D batch normalization layers. At the same time, the recent actions are fed into a stack of two normal LSTM layers. Next, both stacks are concatenated together. The resulting concatenation is then given to a deep neural network containing RELU activations. Finally, this leads to the output layer, which applies a sigmoid function across 9 nodes, where each node again represents each of the different actions available to the player. In this case, the output layer is a prediction of which actions the player will take next.

During training and testing, \textit{y\textsubscript{1}} is always the \textit{y} from the previous frame. During live , however, each value from \textit{y} is converted into either 1 or 0 based on a threshold value. The result of this thresholding is then used as the next \textit{y\textsubscript{1}}.

\begin{figure}
    \caption{High-level overview of the neural network}
    \centering
    \includegraphics[scale = 0.5]{network.png} \\
\end{figure}

%-------------------------------------------%



\section{Features}
%-------------------------------------------%

\textit{Rivals of Aether} has, in total, 9 buttons in its control scheme. These include 4 directional buttons for movement, a jump button, a dodge button, and buttons for 3 types of attacks.

\begin{figure}
	\caption{In-game menu displaying game controls}
	\centering
	\includegraphics[scale = 0.5]{controls.png} \\
\end{figure}

Pressing the left and right directional buttons results in no movement; the same applies to up and down. Otherwise, all button combinations are valid. Thus, the model uses a sigmoid function to predict values between 0 and 1 for each of the 9 buttons. This allows the game agent to use the model's prediction along with an adjustable threshold in order to determine whether or not it should press or release any given button.

%-------------------------------------------%



\section{Test Plan}
%-------------------------------------------%

The full dataset contains a total of 1,032 replay files provided by the Rivals of Aether community. Of those, 222 replay files are unusable because they were recorded in pre-release versions of the game that do not appear to normally support replay recording or playback. Given that the replays exist in the first place, it is conceivable that the functionality exists via an undocumented developer menu or console command. No such accesses were found, however. With the unusable 222 replay files removed, the dataset is left with only 810 replays. Leaving out 20 percent of the dataset for testing reduces the training set to a meager 641 replays. This is not as poor as it may seem at first, however. Each replay can have a duration anywhere up to 8 minutes. Thus, with the frame collector running at 10 frames per second, this leads to an upper limit of 4,800 frames sampled per replay. As a result, the training set contains a total of 859,249 frames while the testing set has 221,782.

The amount of data can still be increased, however. Replays contain input data for a minimum of 2 players and a maximum of 4. This one-to-many relation from replays to players can be exploited if each combination of a replay and player is treated as a separate video. In other words, the frame buffer for replay \textit{R} can be used not only with the input data for player \textit{\textit{P\textsubscript{1}}} but also with those of players \textit{P\textsubscript{2-4}}. Exploding the dataset in this fashion yields a total of 1271 replays for training and 332 replays for testing. 

During the course of training, the model logs its accuracy and loss metrics using a dataframe. After training is complete, the dataframe is saved as CSV file. A separate dataframe and CSV are used for testing. The training and testing accuracy can be compared with each other in order to reveal overfitting. Observing the change in accuracy over time also shows whether or not additional data is helping the model improve. This can also show whether or not multiple epochs are making a difference.

A properly trained model should demonstrate several qualities other than a high accuracy, however. When used by the game agent, the model should make competent gameplay decisions. This can be tested by placing the agent in matches against bots of varying difficulty. Furthermore, the model should cause the agent to behave in a convincingly human-like manner. This means that it should not get stuck or frozen, and it shouldn't engage in strange behavior such as running back and forth or repeatedly jumping off of the stage. This also means that it should not play unrealistically well, such as with by executing frame-perfect moves throughout an entire match.

%-------------------------------------------%



\section{Criteria and Constraints}
%-------------------------------------------%

The dataset used in this project was contributed by the \textit{Rivals of Aether} subreddit community on \textit{/r/rivalsofaether}. Replays received from the community are considered private information and may not, under any circumstances, be redistributed. Only replays recorded by the researchers may be included as sample data in the public repositories associated with the project. Care must be taken, however, in order to avoid loss of data. The replays should be stored in multiple secure locations including at least one encrypted Cloud storage service.

The data processing pipeline constitutes the most significant portion of this project. The machine learning implementation is only a proof of concept, and the game agent's gaming competence should not be used to measure the success of the project.

%-------------------------------------------%
